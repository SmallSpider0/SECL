{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 系统库\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gc\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import argparse\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "# 自定义库\n",
    "from utils.contract import Contract\n",
    "from utils.models import mlp, resnet18\n",
    "from utils.dataset import get_data_loaders\n",
    "\n",
    "\n",
    "# 禁用核心转储文件\n",
    "import resource\n",
    "\n",
    "resource.setrlimit(resource.RLIMIT_CORE, (0, 0))\n",
    "\n",
    "\n",
    "# 随机种子设置\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # 让显卡产生的随机数一致\n",
    "    torch.cuda.manual_seed_all(\n",
    "        seed\n",
    "    )  # 多卡模式下，让所有显卡生成的随机数一致？这个待验证\n",
    "    np.random.seed(seed)  # numpy产生的随机数一致\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters occupy 2.55 MB\n",
      "Model parameters occupy 42.65 MB\n"
     ]
    }
   ],
   "source": [
    "def get_model_size(model):\n",
    "    total_size = 0\n",
    "    for param in model.parameters():\n",
    "        # param.data.numpy().nbytes gives the number of bytes for each parameter tensor\n",
    "        param_size = param.numel() * param.element_size()  # param.numel() gives the number of elements\n",
    "        total_size += param_size\n",
    "    return total_size\n",
    "\n",
    "model_size = get_model_size(mlp())\n",
    "print(f\"Model parameters occupy {model_size / (1024**2):.2f} MB\")\n",
    "\n",
    "model_size = get_model_size(resnet18(num_classes=10))\n",
    "print(f\"Model parameters occupy {model_size / (1024**2):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollaborativeTrain:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_name,\n",
    "        model_name,\n",
    "        device,\n",
    "        lr=0.01,\n",
    "        momentum=0.9,\n",
    "        local_epochs=1,\n",
    "        veri_repeat=1,\n",
    "    ):\n",
    "        self.train_loaders, self.test_loader = None, None\n",
    "        self.dataset_name, self.model_name = dataset_name, model_name\n",
    "        self.lr, self.momentum = lr, momentum\n",
    "        self.local_epochs = 1\n",
    "        self.device = device\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.local_models = None\n",
    "        self.trianer_malicious = []\n",
    "        self.veri_repeat = veri_repeat\n",
    "\n",
    "        # 首次运行初始化全局模型\n",
    "        if self.model_name == \"MLP\" and self.dataset_name == \"MNIST\":\n",
    "            self.global_model = mlp().to(self.device)\n",
    "        elif self.model_name == \"ResNet18\" and self.dataset_name == \"CIFAR10\":\n",
    "            self.global_model = resnet18(num_classes=10).to(self.device)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported model or dataset combination\")\n",
    "\n",
    "    def model_distance(self, model1, model2):\n",
    "        # 将模型参数转换为一维向量并计算差值\n",
    "        differences = [\n",
    "            p1 - p2 for p1, p2 in zip(model1.parameters(), model2.parameters())\n",
    "        ]\n",
    "        # 将差值转换为GPU张量并计算范数\n",
    "        distance = torch.norm(torch.cat([diff.flatten() for diff in differences]))\n",
    "        return distance.item()\n",
    "\n",
    "    # 模型聚合函数\n",
    "    def aggregate_models(self, models, is_malicious=None):\n",
    "        if is_malicious is None:\n",
    "            is_malicious = [False for _ in range(len(models))]\n",
    "\n",
    "        # 获取全局模型的状态字典\n",
    "        global_dict = self.global_model.state_dict()\n",
    "\n",
    "        # 对所有本地模型的参数进行聚合，仅聚合is_malicious为False的模型\n",
    "        for k in global_dict.keys():\n",
    "            model_params = [\n",
    "                models[i].state_dict()[k].float()\n",
    "                for i in range(len(models))\n",
    "                if not is_malicious[i]\n",
    "            ]\n",
    "            if model_params:  # 确保model_params不为空\n",
    "                global_dict[k] = torch.stack(model_params, 0).mean(0)\n",
    "            else:\n",
    "                print(f\"No non-malicious models found for parameter {k}.\")\n",
    "\n",
    "        # 创建一个新的全局模型并加载聚合后的参数\n",
    "        if self.model_name == \"ResNet18\":\n",
    "            aggregated_model = resnet18(num_classes=10).to(self.device)\n",
    "        else:\n",
    "            aggregated_model = self.global_model.__class__().to(self.device)\n",
    "\n",
    "        aggregated_model.load_state_dict(global_dict)\n",
    "        return aggregated_model\n",
    "\n",
    "    # 评估测试集准确率\n",
    "    def test(self, model):\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in self.test_loader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                output = model(data)\n",
    "                test_loss += self.criterion(output, target).item()  # sum up batch loss\n",
    "                pred = output.argmax(\n",
    "                    dim=1, keepdim=True\n",
    "                )  # get the index of the max log-probability\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(self.test_loader.dataset)\n",
    "        accuracy = 100.0 * correct / len(self.test_loader.dataset)\n",
    "        return accuracy, test_loss\n",
    "\n",
    "    # behaviours可选: ADVERSARIAL/FREERIDER/NORMAL\n",
    "    def step_training(self, selections, behaviours):\n",
    "        self.trianer_malicious = [False if b == \"NORMAL\" else True for b in behaviours]\n",
    "\n",
    "        def _local_train(weights):\n",
    "            # 初始化数据集\n",
    "            self.train_loaders, self.test_loader = get_data_loaders(\n",
    "                self.dataset_name, 32, \"./data\", weights\n",
    "            )\n",
    "\n",
    "            # 模拟各个训练者本地训练，并保存训练结果\n",
    "            local_models = []\n",
    "            for i in range(len(weights)):\n",
    "                # 初始化本地模型\n",
    "                if self.model_name == \"ResNet18\":\n",
    "                    local_model = resnet18(num_classes=10).to(self.device)\n",
    "                else:\n",
    "                    local_model = self.global_model.__class__().to(self.device)\n",
    "\n",
    "                if behaviours[i] == \"ADVERSARIAL\":  # 返回随机结果\n",
    "                    local_models.append(local_model)\n",
    "                elif behaviours[i] == \"FREERIDER\":  # 直接返回全局模型\n",
    "                    local_model.load_state_dict(self.global_model.state_dict())\n",
    "                    local_models.append(local_model)\n",
    "                else:  # 正常训练\n",
    "                    local_model.load_state_dict(self.global_model.state_dict())\n",
    "                    optimizer = optim.SGD(\n",
    "                        local_model.parameters(), lr=self.lr, momentum=self.momentum\n",
    "                    )\n",
    "                    # 执行本地训练\n",
    "                    local_model.train()\n",
    "                    for e in range(self.local_epochs):\n",
    "                        for data, target in self.train_loaders[i]:\n",
    "                            data, target = data.to(self.device), target.to(self.device)\n",
    "                            optimizer.zero_grad()\n",
    "                            output = local_model(data)\n",
    "                            loss = self.criterion(output, target)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                    local_models.append(local_model)\n",
    "\n",
    "            # 返回所有本地训练结果\n",
    "            return local_models\n",
    "\n",
    "        # 根据签署的合约 使用特定数据量训练（可以按合约中数据量参数作为权重，划分数据集）\n",
    "        weights = [int(item[1][0]) for item in selections]\n",
    "\n",
    "        # 本地训练一轮\n",
    "        self.local_models = _local_train(weights)\n",
    "        return weights\n",
    "\n",
    "    def step_verification(self):\n",
    "        # TODO：这里需要添加验证代码\n",
    "\n",
    "        def veri_0_none():\n",
    "            # 检测不出任何攻击\n",
    "            return \"none\"\n",
    "\n",
    "        \"\"\"\n",
    "        测试1：评估各trainer提交的测试集准确率（并与现有全局模型对比）\n",
    "        检测攻击原理：\n",
    "        - 测试集准确率相对前一个全局模型的提升越高，说明该提交对全局模型的贡献越大，而提升过小（甚至负数）的提交被认为是恶意的\n",
    "        阈值设置：\n",
    "        \"\"\"\n",
    "\n",
    "        def veri_1_testacc():\n",
    "            ret = []\n",
    "            acc_global, _ = self.test(self.global_model)\n",
    "            for model in self.local_models:\n",
    "                accuracy, _ = self.test(model)\n",
    "                ret.append(accuracy - acc_global)\n",
    "\n",
    "            # 判断参与者是否恶意\n",
    "            ret = np.array(ret)\n",
    "            return ret\n",
    "\n",
    "        \"\"\"\n",
    "        测试2：基于Shapley值计算各训练者贡献度\n",
    "        缺陷：计算开销很大（参与者指数级别开销）\n",
    "        阈值设置：\n",
    "        参考：https://github.com/clickade/federated-shapley-playground\n",
    "        \"\"\"\n",
    "\n",
    "        def veri_2_shapley(num_samples=10):\n",
    "            n = len(self.local_models)\n",
    "            marginal_contributions = np.zeros(n)\n",
    "            permutations = [\n",
    "                random.sample(range(n), n) for _ in range(num_samples)\n",
    "            ]  # 随机采样排列\n",
    "\n",
    "            for perm in permutations:\n",
    "                current_value = 0\n",
    "                previous_value = 0\n",
    "                for i in range(len(perm)):\n",
    "                    subset_models = [self.local_models[j] for j in perm[: i + 1]]\n",
    "                    aggregated_model = self.aggregate_models(subset_models)\n",
    "                    accuracy, _ = self.test(aggregated_model)\n",
    "                    current_value = accuracy\n",
    "                    marginal_contributions[perm[i]] += current_value - previous_value\n",
    "                    previous_value = current_value\n",
    "\n",
    "            shapley_values = marginal_contributions / num_samples\n",
    "            return shapley_values\n",
    "\n",
    "        \"\"\"\n",
    "        测试3：基于influence metric计算各训练者贡献度\n",
    "        检测攻击原理：\n",
    "        - inﬂuence越大，说明该提交对全局模型的贡献越大，而infulence过小（如负数）的提交被认为是恶意的\n",
    "\n",
    "\n",
    "        阈值设置：\n",
    "        参考：The inﬂuence [9] of a data point is deﬁned as the diﬀerence in loss function between the model trained with and without the data point.\n",
    "        \"\"\"\n",
    "\n",
    "        def veri_3_influence():\n",
    "            # TODO： 判断infulence计算是否有误\n",
    "            n = len(self.local_models)\n",
    "            influence_contributions = np.zeros(n)\n",
    "\n",
    "            # 计算基线模型的损失\n",
    "            full_model = self.aggregate_models(self.local_models)\n",
    "            _, baseline_loss = self.test(full_model)\n",
    "\n",
    "            # 对每个训练者模型计算影响力\n",
    "            for i in range(n):\n",
    "                # 移除第 i 个训练者模型，计算新的聚合模型损失\n",
    "                subset_models = [self.local_models[j] for j in range(n) if j != i]\n",
    "                aggregated_model = self.aggregate_models(subset_models)\n",
    "                _, loss = self.test(aggregated_model)\n",
    "                # 影响力为基线损失与当前损失之差\n",
    "                influence_contributions[i] = -(baseline_loss - loss)\n",
    "            return influence_contributions\n",
    "\n",
    "        \"\"\"\n",
    "        测试4：基于Multi-KRUM算法计算各训练者贡献度\n",
    "        参考：The veriﬁer will add up Euclidean distances of each customer i’s update to the closest R − f − 2 updates \n",
    "        and denote the sum as each customer i’s score s(i). R means the number of updates, and f means the number of Byzantine customers.\n",
    "        \"\"\"\n",
    "\n",
    "        def veri_4_multi_KRUM():\n",
    "            n = len(self.local_models)\n",
    "            f = math.ceil(n / 2) - 1\n",
    "            scores = np.zeros(n)\n",
    "\n",
    "            for i in range(n):\n",
    "                # 计算更新i到其它所有更新的距离\n",
    "                distances = []\n",
    "                for j in range(n):\n",
    "                    if i != j:\n",
    "                        distances.append(\n",
    "                            self.model_distance(\n",
    "                                self.local_models[i], self.local_models[j]\n",
    "                            )\n",
    "                        )\n",
    "                # 计算每个更新的得分（最小的n - f - 2个距离之和）\n",
    "                distances.sort()\n",
    "                # 返回负值 距离越大则分数越低\n",
    "                scores[i] = -sum(distances[: n - f - 2])\n",
    "            return scores\n",
    "\n",
    "        \"\"\"\n",
    "        测试5：基于update significance计算各训练者贡献度\n",
    "        检测方法：update signiﬁcance过大的提交被认为是恶意\n",
    "        参考：In this paper, the update signiﬁcance is measured by model deviation, \n",
    "        which is the divergence of a particular local model from the average across all local models [20], [21].\n",
    "        \"\"\"\n",
    "\n",
    "        def veri_5_update_significance():\n",
    "            n = len(self.local_models)\n",
    "            # 计算所有本地模型参数的平均值\n",
    "            avg_model_params = {\n",
    "                name: torch.zeros_like(param.data)\n",
    "                for name, param in self.local_models[0].named_parameters()\n",
    "            }\n",
    "            for model in self.local_models:\n",
    "                for name, param in model.named_parameters():\n",
    "                    avg_model_params[name] += param.data / n\n",
    "\n",
    "            # 计算每个本地模型与平均模型之间的欧几里得距离\n",
    "            update_significance = []\n",
    "            for model in self.local_models:\n",
    "                deviation = 0.0\n",
    "                for name, param in model.named_parameters():\n",
    "                    deviation += (\n",
    "                        torch.norm(param.data - avg_model_params[name]).item() ** 2\n",
    "                    )\n",
    "                deviation = deviation**0.5\n",
    "                update_significance.append(deviation)\n",
    "\n",
    "            return update_significance\n",
    "\n",
    "        def veri_6_reproduction():\n",
    "            # 初始化本地模型\n",
    "            if self.model_name == \"ResNet18\":\n",
    "                local_model = resnet18(num_classes=10).to(self.device)\n",
    "            else:\n",
    "                local_model = self.global_model.__class__().to(self.device)\n",
    "\n",
    "            for index_model, train_loader in enumerate(self.train_loaders):\n",
    "                # 计算出需要训练的batch数量，并模拟训练\n",
    "                # 读取每一个本地模型\n",
    "                local_model.load_state_dict(self.local_models[index_model].state_dict())\n",
    "                optimizer = optim.SGD(\n",
    "                    local_model.parameters(), lr=self.lr, momentum=self.momentum\n",
    "                )\n",
    "                # 执行本地训练\n",
    "                local_model.train()\n",
    "                for batch_num, (data, target) in enumerate(train_loader):\n",
    "                    if batch_num == 15:\n",
    "                        break\n",
    "                    data, target = data.to(self.device), target.to(self.device)\n",
    "                    optimizer.zero_grad()\n",
    "                    output = local_model(data)\n",
    "                    loss = self.criterion(output, target)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            # 检测出所有\n",
    "            return \"all\"\n",
    "\n",
    "        # 验证方法列表\n",
    "        methods = [\n",
    "            (\"test_acc\", veri_1_testacc),\n",
    "            (\"shapley\", veri_2_shapley),\n",
    "            (\"influence\", veri_3_influence),\n",
    "            (\"multi_KRUM\", veri_4_multi_KRUM),\n",
    "            (\"reproduction\", veri_6_reproduction),\n",
    "        ]\n",
    "\n",
    "        # 实验1：评估验证方法执行时间\n",
    "        veri_time = []\n",
    "        for method in methods:\n",
    "            tmp = []\n",
    "            for _ in range(self.veri_repeat):\n",
    "                start = time.time()\n",
    "                scores = method[1]()\n",
    "                if not isinstance(scores, str):\n",
    "                    threshold = np.mean(scores) - 1 * np.std(scores)\n",
    "                    is_malicious = (scores <= threshold).tolist()\n",
    "                elif scores == \"all\":\n",
    "                    # 用我们的复现检测 能检测到所有攻击\n",
    "                    is_malicious = self.trianer_malicious\n",
    "                time_elapsed = time.time() - start\n",
    "                tmp.append(time_elapsed)\n",
    "                print(method[0], f\"{time_elapsed:.2f}\")\n",
    "            veri_time.append(tmp)\n",
    "\n",
    "        return veri_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task1\n",
      "test_acc 35.75 1.243\n",
      "shapley 397.86 4.002\n",
      "influence 42.50 1.624\n",
      "multi_KRUM 0.15 0.008\n",
      "reproduction 3.69 0.153\n",
      "task2\n",
      "test_acc 19.94 0.076\n",
      "shapley 196.74 10.484\n",
      "influence 22.79 1.105\n",
      "multi_KRUM 0.02 0.001\n",
      "reproduction 1.42 0.041\n"
     ]
    }
   ],
   "source": [
    "def eval(task, veri_repeat):\n",
    "    def generate_normal_random_numbers(count, mean, std_dev):\n",
    "        random_numbers = np.empty(count)\n",
    "        generated_count = 0\n",
    "\n",
    "        while generated_count < count:\n",
    "            remaining_count = count - generated_count\n",
    "            new_numbers = np.random.normal(\n",
    "                loc=mean, scale=std_dev, size=remaining_count\n",
    "            )\n",
    "            valid_numbers = new_numbers[new_numbers >= 0]\n",
    "            valid_count = len(valid_numbers)\n",
    "            random_numbers[generated_count : generated_count + valid_count] = (\n",
    "                valid_numbers\n",
    "            )\n",
    "            generated_count += valid_count\n",
    "\n",
    "        return random_numbers\n",
    "\n",
    "    # 设置随机数种子\n",
    "    set_seed(42)\n",
    "\n",
    "    # 参与者相关参数参数\n",
    "    # 模拟各类型参与者平均分布\n",
    "    participants_num = 10\n",
    "    estimated_types = np.array([0.1, 0.13, 0.16, 0.19, 0.22])\n",
    "    estimated_props = np.array([0.2, 0.2, 0.2, 0.2, 0.2])\n",
    "\n",
    "    participants = np.array([])\n",
    "    for type, prop in zip(estimated_types, estimated_props):\n",
    "        tmp = generate_normal_random_numbers(int(participants_num * prop), type, 0.01)\n",
    "        participants = np.concatenate((participants, tmp))\n",
    "    participants = np.sort(participants)\n",
    "\n",
    "    # 配置参与者行为【按顺序分配攻击者】\n",
    "    # If client is adversarial, they return randomized parameters （即随机初始化模型并返回）\n",
    "    # If client is a freerider, they return the same server model parameters\n",
    "    adv_num = 0  # 攻击者数量（返回随机结果）\n",
    "    freerider_num = 0  # 偷懒者数量（不训练）\n",
    "    participants_behaviors = (\n",
    "        [\"ADVERSARIAL\"] * adv_num\n",
    "        + [\"FREERIDER\"] * freerider_num\n",
    "        + [\"NORMAL\"] * (participants_num - adv_num - freerider_num)\n",
    "    )\n",
    "\n",
    "    # 任务相关参数\n",
    "    # k1为全局准确率到模型价值的转换参数\n",
    "    # k2和k3为本地训练数据总量到全局模型准确率的转换参数（和训练任务有关，需要根据实际任务调整）\n",
    "    u_m_k = [300, 0.001]\n",
    "    u_p = lambda reward, cost: reward - cost\n",
    "    u_m = lambda data, reward: u_m_k[0] * cp.log(1 + u_m_k[1] * data) - reward\n",
    "\n",
    "    # task1\n",
    "    if task == 1:\n",
    "        dataset_name = \"CIFAR10\"\n",
    "        model_name = \"ResNet18\"\n",
    "    elif task == 2:\n",
    "        dataset_name = \"MNIST\"\n",
    "        model_name = \"MLP\"\n",
    "\n",
    "    CT = CollaborativeTrain(dataset_name, model_name, \"cuda\", veri_repeat=veri_repeat)\n",
    "    C = Contract()\n",
    "\n",
    "    # 步骤1：设计合约\n",
    "    contracts = C.design_contract(estimated_types, u_m, u_p)\n",
    "\n",
    "    # 步骤2：各TN签署合约\n",
    "    selections = C.select_contract(participants, contracts)\n",
    "\n",
    "    # 【仅训练一个global epoch】\n",
    "    st = time.time()\n",
    "    # 步骤3：各TN根据签署的合约本地训练并提交结果\n",
    "    weights = CT.step_training(selections, participants_behaviors)\n",
    "\n",
    "    # 步骤4：MO验证各TN的提交，并获取它们实际贡献/收到的奖励/验证结果\n",
    "    veri_time = CT.step_verification()\n",
    "\n",
    "    print(veri_time)\n",
    "    save_path = f\"./results_overhead/task{task}\"\n",
    "    torch.save(veri_time, save_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # eval(1, 5)\n",
    "    # eval(2, 5)\n",
    "\n",
    "    methods = [\"test_acc\", \"shapley\", \"influence\", \"multi_KRUM\", \"reproduction\"]\n",
    "\n",
    "    data1 = torch.load(\"./results_overhead/task1\")\n",
    "    data2 = torch.load(\"./results_overhead/task2\")\n",
    "\n",
    "    print('task1')\n",
    "    for index, data in enumerate(data1):\n",
    "        print(methods[index], f\"{np.average(data):.2f}\", f\"{np.std(data):.3f}\")\n",
    "\n",
    "    print('task2')\n",
    "    for index, data in enumerate(data2):\n",
    "        print(methods[index], f\"{np.average(data):.2f}\", f\"{np.std(data):.3f}\")\n",
    "\n",
    "\n",
    "# eval(2, 1)\n",
    "# test_acc 24.18\n",
    "# shapley 209.86\n",
    "# influence 24.65\n",
    "# multi_KRUM 0.06\n",
    "# reproduction 1.45\n",
    "\n",
    "# eval(1, 1)\n",
    "# test_acc 40.01\n",
    "# shapley 394.05\n",
    "# influence 44.63\n",
    "# multi_KRUM 0.19\n",
    "# reproduction 4.90"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
